{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f014b2",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "\n",
    "# Yifei Gu\n",
    "# Cindy Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import re\n",
    "import os.path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "TAXI_CSV = \"taxi.csv\"\n",
    "\n",
    "EARTH_REDIUS = 6370.856\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA = [-73.891694, 40.763007, -73.850066, 40.790436]\n",
    "JFK = [-73.829375, 40.617517, -73.743544, 40.670660]\n",
    "EWR = [-74.205452, 40.664547, -74.146572, 40.716221]\n",
    "sample_size = 100\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rad(d):\n",
    "    \"\"\" return the rad for given input\"\"\"\n",
    "    \n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \"\"\" Calculate the distance between from_coord and to_coord\"\"\"\n",
    "    \n",
    "    radLat1 = rad(from_coord['pickup_latitude'])\n",
    "    radLat2 = rad(to_coord['dropoff_latitude'])\n",
    "    a = radLat1 - radLat2\n",
    "    #print('a',a)\n",
    "    b = rad(from_coord['pickup_longitude']) - rad(to_coord['dropoff_longitude'])\n",
    "    #print(b)\n",
    "    distance = 2 * math.asin(math.sqrt(math.pow(math.sin(a.iloc[0] / 2), 2) \\\n",
    "                                       + math.cos(radLat1.iloc[0]) * math.cos(radLat2.iloc[0]) * math.pow(math.sin(b.iloc[0] / 2), 2)))\n",
    "    distance = distance * EARTH_REDIUS\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62aaa784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    \"\"\" Add the calculated distance as a column to the dataframe\"\"\"\n",
    "    \n",
    "    from_coord = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    dataframe['distance'] = calculate_distance(from_coord, to_coord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\" Programmatically download the Yellow Taxi Parquet files\"\"\"\n",
    "    url_list=[]\n",
    "    try:\n",
    "        response=requests.get(TAXI_URL)\n",
    "        if response.status_code==200:\n",
    "            soup=BeautifulSoup(response.content,'lxml')\n",
    "            hrefs=soup.find_all('a',href=re.compile(\"yellow_tripdata\"))\n",
    "            for href in hrefs:\n",
    "                url=href.get('href')\n",
    "                date = url.split('/')[-1].split(\"_\")[-1]\n",
    "                year = int(date[:4])\n",
    "                if year >= 2009 and year < 2015:\n",
    "                    url_list.append(url)\n",
    "                if year == 2015:\n",
    "                    month = int(date[5:7])\n",
    "                    if month <= 6:\n",
    "                        url_list.append(url) \n",
    "            #print(url_list)\n",
    "        return url_list\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"tpep_pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"tip_amount\",\n",
    "           \"total_amount\"]\n",
    "columns2 = [\"pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"tip_amount\",\n",
    "           \"total_amount\"]\n",
    "columns3 = [\"Trip_Pickup_DateTime\", \n",
    "           \"Start_Lon\",\n",
    "           \"Start_Lat\",\n",
    "           \"End_Lon\", \n",
    "           \"End_Lat\",\n",
    "           \"Tip_Amt\",\n",
    "           \"Total_Amt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlog_column(dataframe):\n",
    "    \"\"\" Add latitude and logtitude as columns to the dataframe for convenient calculation\"\"\"\n",
    "    print('add_latlog_column')\n",
    "    dftaxi = gpd.read_file('taxi_zones.shp')\n",
    "    dftaxi = dftaxi.to_crs(epsg=4326)\n",
    "    \n",
    "    lat1=[]\n",
    "    log1=[]\n",
    "    lat2=[]\n",
    "    log2=[]\n",
    "    for LocationID in dataframe[\"PULocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        log=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat1.append(0)\n",
    "        else:\n",
    "            lat1.append(lat.iloc[0])\n",
    "        if log.empty:\n",
    "            log1.append(0)\n",
    "        else:\n",
    "            log1.append(log.iloc[0])\n",
    "    for LocationID in dataframe[\"DOLocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        log=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat2.append(0)\n",
    "        else:\n",
    "            lat2.append(lat.iloc[0])\n",
    "        if log.empty:\n",
    "            log2.append(0)\n",
    "        else:\n",
    "            log2.append(log.iloc[0])\n",
    "    dataframe['pickup_latitude']=lat1\n",
    "    dataframe['pickup_longitude']=log1\n",
    "    dataframe['dropoff_latitude']=lat2\n",
    "    dataframe['dropoff_longitude']=log2\n",
    "    dataframe.to_csv(\"2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94250b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \"\"\" Clean the dataframe according to existing location IDs, and return\"\"\"\n",
    "    reponse = requests.get(url)\n",
    "\n",
    "    filename=url.split('/')[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(reponse.content)\n",
    "    \n",
    "    df = pd.read_parquet(filename)\n",
    "    print(filename)\n",
    "    print(df.columns)\n",
    "    df = df.sample(n=sample_size,ignore_index=True)\n",
    "    '''\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(df.columns)\n",
    "   \n",
    "    df=df.sample(n=sample_size,ignore_index=True)\n",
    "    add_latlog_column(df)\n",
    "    df = df[columns]\n",
    "    df.columns = columns2\n",
    "    df = df[columns2]\n",
    "    '''\n",
    "    try:\n",
    "        if \"PULocationID\" in df.columns:\n",
    "            add_latlog_column(df)\n",
    "        df = df[columns]\n",
    "    except:\n",
    "        try:\n",
    "            df = df[columns2]\n",
    "        except:\n",
    "            try:\n",
    "                df = df[columns3]\n",
    "            except:\n",
    "                add_latlog_column(df)\n",
    "                df = df[columns2]\n",
    "    df.columns = columns2\n",
    "   # df.to_csv(\"3.csv\")\n",
    "    #print(df[\"pickup_longitude\"])\n",
    "    '''\n",
    "    df=df[df[\"pickup_longitude\"] > -74.242330]\n",
    "    df = df[(df[\"pickup_longitude\"] > -74.242330) \n",
    "           &(df[\"pickup_longitude\"] < -73.717047)\n",
    "           & (df[\"pickup_latitude\"] > 40.560445) \n",
    "           & (df[\"pickup_latitude\"] < 40.908524)\n",
    "    '''\n",
    "    df = df.sample(n=sample_size,random_state = 1,ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a1467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    \"\"\" Clean the overall dataframe, and return\"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    if os.path.exists(TAXI_CSV):\n",
    "        taxi_data=pd.read_csv(TAXI_CSV)\n",
    "    else:\n",
    "        all_csv_urls = find_taxi_csv_urls()\n",
    "        for csv_url in all_csv_urls[0:2]:\n",
    "            dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "            add_distance_column(dataframe)\n",
    "            all_taxi_dataframes.append(dataframe)\n",
    "        print(all_taxi_dataframes)\n",
    "        taxi_data = pd.concat(all_taxi_dataframes)\n",
    "        taxi_data.to_csv(TAXI_CSV)\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\" Load the uber data into dataframe,filter data, and reformat columns to desired name\"\"\"\n",
    "    df=pd.read_csv(csv_file)\n",
    "    df=df[(df[\"pickup_longitude\"] > -74.242330) \n",
    "           &(df[\"pickup_longitude\"] < -73.717047)\n",
    "           & (df[\"pickup_latitude\"] > 40.560445) \n",
    "           & (df[\"pickup_latitude\"] < 40.908524)]\n",
    "    print(df)\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df[\"dayofweek\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    df[\"year\"] = df[\"pickup_datetime\"].dt.year\n",
    "    df[\"month\"] = df[\"pickup_datetime\"].dt.month\n",
    "    df[\"day\"] = df[\"pickup_datetime\"].dt.day\n",
    "    df[\"hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\" Load cleaned uber data into dataframe, and add distance column, and return\"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\" Load weather data and clean the dataframe to generate hourly weather data\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[['DATE', 'HourlyWindSpeed', 'HourlyPrecipitation']]\n",
    "    df = df.dropna() \n",
    "    #df.to_csv(\"12.csv\")\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'].replace(['T'], ''), errors='coerce')\n",
    "    df['HourlyWindSpeed'] = pd.to_numeric(df['HourlyWindSpeed'].replace(['T'], ''), errors='coerce')\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    #df.to_csv(\"121.csv\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\" Load weather data, clean and calculate from the dataframe to generate daily weather data\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[['HourlyWindSpeed','HourlyPrecipitation','DATE']]\n",
    "    df = df.dropna()  \n",
    "    #df.to_csv(\"13.csv\")\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'].replace(['T'], ''), errors='coerce')\n",
    "    df['HourlyWindSpeed'] = pd.to_numeric(df['HourlyWindSpeed'].replace(['T'], ''), errors='coerce')\n",
    "    df['DATE'] = pd.to_datetime(df['DATE']).dt.strftime('%Y-%m-%d')\n",
    "    daily_df = df.groupby(['DATE']).agg({'HourlyWindSpeed': 'mean', 'HourlyPrecipitation': 'sum'})\n",
    "    daily_df.rename(columns = {'HourlyWindSpeed':'DailyWindSpeed', 'HourlyPrecipitation':'DailyPrecipitation'}, inplace = True)\n",
    "    #daily_df.to_csv(\"131.csv\")\n",
    "    return daily_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\" Load weather data, and generate hourly_dataframes and daily_dataframes and return for future calculation\"\"\"\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files =glob.glob(\"*weather*.csv\") \n",
    "    print(weather_csv_files) \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                            key  fare_amount  \\\n",
      "0         24238194    2015-05-07 19:52:06.0000003          7.5   \n",
      "1         27835199    2009-07-17 20:04:56.0000002          7.7   \n",
      "2         44984355   2009-08-24 21:45:00.00000061         12.9   \n",
      "3         25894730    2009-06-26 08:22:21.0000001          5.3   \n",
      "4         17610152  2014-08-28 17:47:00.000000188         16.0   \n",
      "...            ...                            ...          ...   \n",
      "199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n",
      "199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n",
      "199997    27804658   2009-06-29 00:42:00.00000078         30.9   \n",
      "199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n",
      "199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n",
      "\n",
      "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
      "0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
      "1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
      "2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
      "3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
      "4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
      "...                         ...               ...              ...   \n",
      "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
      "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
      "199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n",
      "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
      "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
      "\n",
      "        dropoff_longitude  dropoff_latitude  passenger_count  \n",
      "0              -73.999512         40.723217                1  \n",
      "1              -73.994710         40.750325                1  \n",
      "2              -73.962565         40.772647                1  \n",
      "3              -73.965316         40.803349                3  \n",
      "4              -73.973082         40.761247                5  \n",
      "...                   ...               ...              ...  \n",
      "199995         -73.986525         40.740297                1  \n",
      "199996         -74.006672         40.739620                1  \n",
      "199997         -73.858957         40.692588                2  \n",
      "199998         -73.983215         40.695415                1  \n",
      "199999         -73.985508         40.768793                1  \n",
      "\n",
      "[195825 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"                                   \n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(                                  \n",
    "   DATE timestamp PRIMARY KEY,                                \n",
    "   HourlyWindSpeed FLOAT,                                     \n",
    "   HourlyPrecipitation FLOAT                                  \n",
    ");                                                            \n",
    "\"\"\"                                                           \n",
    "                                                              \n",
    "DAILY_WEATHER_SCHEMA = \"\"\"                                    \n",
    "CREATE TABLE IF NOT EXISTS daily_weather(                                   \n",
    "   DATE timestamp PRIMARY KEY,                                \n",
    "   DailyWindSpeed FLOAT,                                      \n",
    "   DailyPrecipitation FLOAT                                   \n",
    ");                                                            \n",
    "\"\"\"                                                           \n",
    "                              \n",
    "TAXI_TRIPS_SCHEMA = \"\"\"       \n",
    "CREATE TABLE IF NOT EXISTS taxis_trips(     \n",
    "   pickup_datetime timestamp, \n",
    "   tip_amount FLOAT,          \n",
    "   pickup_longitude DOUBLE,   \n",
    "   pickup_latitude DOUBLE,    \n",
    "   dropoff_longitude DOUBLE,  \n",
    "   dropoff_latitude DOUBLE,   \n",
    "   distance DOUBLE            \n",
    ");                            \n",
    "\"\"\"                           \n",
    "                              \n",
    "UBER_TRIPS_SCHEMA = \"\"\"       \n",
    "CREATE TABLE IF NOT EXISTS uber_trips(      \n",
    "   pickup_datetime timestamp, \n",
    "   pickup_longitude DOUBLE,   \n",
    "   pickup_latitude DOUBLE,    \n",
    "   dropoff_longitude DOUBLE,  \n",
    "   dropoff_latitude DOUBLE,   \n",
    "   distance DOUBLE            \n",
    ");                            \n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\" Load the data in dataframes to tables\"\"\"\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(name=table_name, con=engine.connect(), index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a444ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test tables\"\"\"\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"select * from taxis_trips limit 10\").fetchall()\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "    result = connection.execute(\"select * from uber_trips limit 10\").fetchall()\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "    result = connection.execute(\"select * from hourly_weather limit 10\").fetchall()\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "    result = connection.execute(\"select date from daily_weather limit 10\").fetchall()\n",
    "    for row in result:\n",
    "        print(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    \"\"\" Write the given query into the output file\"\"\"\n",
    "    with open(outfile, \"w+\") as f:\n",
    "        f.write(query)\n",
    "        f.write('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT STRFTIME('%H', pickup_datetime) as hour,\n",
    "       COUNT(*) as ORDERPERHOUR\n",
    "FROM taxi_trips\n",
    "GROUP BY STRFTIME('%H', pickup_datetime)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=engine.execute(QUERY_1).fetchall()\n",
    "for row in result:\n",
    "    print(row)      \n",
    "write_query_to_file(QUERY_1, \"QUERY_1.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d09011",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT STRFTIME('%w', pickup_datetime) as weekday,\n",
    "       COUNT(*) as DAROFWEEK\n",
    "FROM uber_trips\n",
    "GROUP BY STRFTIME('%w', pickup_datetime)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba382fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=engine.execute(QUERY_2).fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"QUERY_2.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_31=\"\"\"\n",
    "CREATE TABLE  IF NOT EXISTS hired_trips  as \n",
    "SELECT pickup_datetime , distance \n",
    "FROM taxi_trips WHERE STRFTIME('%Y', pickup_datetime) = 2013 AND STRFTIME('%m', pickup_datetime) = '07'\n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance \n",
    "FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = 2013 AND STRFTIME('%m', pickup_datetime) = '07';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_32 = \"\"\"\n",
    "SELECT\n",
    "  distance AS '95% distance'\n",
    "FROM hired_trips\n",
    "ORDER BY distance ASC\n",
    "LIMIT 1\n",
    "OFFSET (SELECT\n",
    "         COUNT(*)\n",
    "        FROM hired_trips) * 95 / 100 - 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0611f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"DROP TABLE  IF EXISTS  hired_trips\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_31)\n",
    "result=engine.execute(QUERY_32).fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_31+\"\\n\"+QUERY_32, \"QUERY_3.sql\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_41=\"\"\"\n",
    "CREATE TABLE  IF NOT EXISTS hired_trips  as \n",
    "SELECT pickup_datetime , distance \n",
    "FROM taxi_trips WHERE STRFTIME('%Y', pickup_datetime) = 2009 \n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance \n",
    "FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = 2009 ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d158f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_42 = \"\"\"\n",
    "SELECT STRFTIME('%j', pickup_datetime),AVG(distance) from hired_trips GROUP BY STRFTIME('%j', pickup_datetime) \n",
    "HAVING COUNT(*) IN (  SELECT DISTINCT COUNT(*)\n",
    "  FROM hired_trips data\n",
    "  GROUP BY STRFTIME('%j', pickup_datetime)\n",
    "  ORDER BY count(*) DESC\n",
    "  LIMIT 10);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2473bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"DROP TABLE  IF EXISTS  hired_trips\")\n",
    "engine.execute(QUERY_41)\n",
    "result=engine.execute(QUERY_42).fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_51 = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hired_trips as \n",
    "SELECT pickup_datetime , distance \n",
    "FROM taxi_trips WHERE STRFTIME('%Y', pickup_datetime) = 2014\n",
    "UNION ALL\n",
    "SELECT pickup_datetime, distance \n",
    "FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = 2014;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_52 = \"\"\"\n",
    "SELECT STRFTIME('%j', pickup_datetime), COUNT(*) from hired_trips GROUP BY STRFTIME('%j', pickup_datetime) \n",
    "HAVING STRFTIME('%j', pickup_datetime) IN (\n",
    "  SELECT STRFTIME('%j', DATE) from daily_weather\n",
    "  ORDER BY DailyWindSpeed  DESC\n",
    "  LIMIT 10\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"DROP TABLE  IF EXISTS  hired_trips\")\n",
    "engine.execute(QUERY_51)\n",
    "result=engine.execute(QUERY_52).fetchall()\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_61 =\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hired_trips as \n",
    "SELECT STRFTIME('%j', pickup_datetime) as DATE, STRFTIME('%H', pickup_datetime) as HOUR from \n",
    "(SELECT pickup_datetime \n",
    "FROM taxi_trips WHERE STRFTIME('%Y', pickup_datetime) = 2012 AND \n",
    "STRFTIME('%m', pickup_datetime) = '10' AND STRFTIME('%j', pickup_datetime) <= '30'  AND STRFTIME('%j', pickup_datetime) <= '22'\n",
    "UNION ALL\n",
    "SELECT pickup_datetime\n",
    "FROM uber_trips WHERE STRFTIME('%Y', pickup_datetime) = 2012 AND STRFTIME('%m', pickup_datetime) = '10' \n",
    "AND STRFTIME('%j', pickup_datetime) <= '30'  AND STRFTIME('%j', pickup_datetime) <= '22');\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QUERY_62 =\"\"\"\n",
    "SELECT COUNT(*), HourlyPrecipitation, HourlyWindSpeed from hired_trips INNER JOIN hourly_weather \n",
    "ON hired_trips.DATE = STRFTIME('%j', hourly_weather.DATE) \n",
    "AND hired_trips.HOUR =STRFTIME('%h', hourly_weather.DATE)\n",
    "GROUP BY hourly_weather.DATE, STRFTIME('%h', hourly_weather.DATE);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63baf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(\"DROP TABLE  IF EXISTS  hired_trips\")\n",
    "#engine.execute(QUERY_60)\n",
    "engine.execute(QUERY_61)\n",
    "result=engine.execute(QUERY_62).fetchall()\n",
    "for row in result:\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7959386",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_61+\"\\n\"+QUERY_62, \"QUERY_6.sql\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    plt.title(\"Number of ORDERPERHOUR in Each Hour\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Number of ORDERPERHOUR\")\n",
    "    plt.plot(dataframe['HOUR'],dataframe['ORDERPERHOUR'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=engine.execute(QUERY_1).fetchall()\n",
    "df=pd.DataFrame(result)\n",
    "df.columns = ['HOUR','ORDERPERHOUR']\n",
    "print(df)\n",
    "print(df.iloc[:,0])\n",
    "plot_visual_1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_2(dataframe):\n",
    "    x = dataframe[\"month\"]\n",
    "    y = dataframe[\"ave_distance\"]\n",
    "    ci = 1.645 * dataframe[\"ave_distance\"]/np.sqrt(dataframe[\"count\"])\n",
    "    plt.plot(x, y)\n",
    "    plt.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "    plt.title(\"Average distance in each month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average distance\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    month, AVG(distance) AS ave_distance,\n",
    "    COUNT(*) as count\n",
    "FROM (SELECT STRFTIME('%m', pickup_datetime) as month , distance \n",
    "FROM taxi_trips \n",
    "UNION ALL\n",
    "SELECT  STRFTIME('%m', pickup_datetime) as month, distance\n",
    "FROM uber_trips )\n",
    "GROUP BY month\n",
    "\"\"\"\n",
    "result=engine.execute(QUERY_2).fetchall()\n",
    "df=pd.DataFrame(result)\n",
    "df.columns = ['month','ave_distance','count']\n",
    "plot_visual_2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_3(lga_rides,jfk_rides,ewr_rides):\n",
    "    plt.plot(lga_rides[\"day\"], lga_rides[\"num_airports\"], label=\"LGA\")\n",
    "    plt.plot(jfk_rides[\"day\"], jfk_rides[\"num_airports\"], label=\"JFK\")\n",
    "    plt.plot(ewr_rides[\"day\"], ewr_rides[\"num_airports\"], label=\"EWR\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Number of rides in day of the week\")\n",
    "    plt.xlabel(\"Day of the week\")\n",
    "    plt.ylabel(\"Number of rides\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_airports = pd.concat([taxi_data, uber_data], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBox(df, box):\n",
    "    df = df[\n",
    "         (df[\"dropoff_longitude\"] > box[0]) \n",
    "       & (df[\"dropoff_longitude\"] < box[2]) \n",
    "       & (df[\"dropoff_latitude\"] > box[1]) \n",
    "       & (df[\"dropoff_latitude\"] < box[3])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lga_airports = filterBox(all_airports, LGA)\n",
    "jfk_airports = filterBox(all_airports, JFK)\n",
    "ewr_airports = filterBox(all_airports, EWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"lga_airports\": lga_airports,\n",
    "    \"jfk_airports\": jfk_airports,\n",
    "    \"ewr_airports\": ewr_airports\n",
    "}  \n",
    "write_dataframes_to_table(map_table_name_to_dataframe) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT STRFTIME('%w', pickup_datetime) AS day, COUNT(*) as num_airports\n",
    "FROM lga_airports\n",
    "GROUP BY STRFTIME('%w', pickup_datetime)\n",
    "\"\"\"\n",
    "result=engine.execute(query).fetchall()\n",
    "lga_airports = pd.DataFrame(result)\n",
    "lga_airports.columns = ['day','num_airports']\n",
    "query = \"\"\"\n",
    "SELECT STRFTIME('%w', pickup_datetime)  AS day, COUNT(*) as num_airports\n",
    "FROM jfk_airports\n",
    "GROUP BY STRFTIME('%w', pickup_datetime)\n",
    "\"\"\"\n",
    "result=engine.execute(query).fetchall()\n",
    "jfk_airports = pd.DataFrame(result)\n",
    "jfk_airports.columns = ['day','num_airports']\n",
    "query = \"\"\"\n",
    "SELECT STRFTIME('%w', pickup_datetime) AS day, COUNT(*) as num_airports\n",
    "FROM ewr_airports\n",
    "GROUP BY STRFTIME('%w', pickup_datetime)\n",
    "\"\"\"\n",
    "result=engine.execute(query).fetchall()\n",
    "ewr_airports = pd.DataFrame(result)\n",
    "ewr_airports.columns = ['day','num_airports']\n",
    "plot_visual_3(lga_airports,jfk_airports,ewr_airports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ca972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8a85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
